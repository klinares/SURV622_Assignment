---
title: "DOGE Days on Reddit: Decoding Public Sentiment in a Federal Shakeup"
format:
  jasa-pdf:
    keep-tex: true  
    journal:
      blinded: false
  jasa-html: default
date: last-modified
author:
  - name: Kevin Linares
    affiliations:
      - name: University of Maryland
  - name: Felix Baez-Santiago
  - name: Aria Lu
  - name: Gloria Zhou
    affiliations:
      - name: University of Michigan
abstract: |
  This study explores the impact of the Department of Government Efficiency (DOGE)'s federal workforce reductions, known as reduction in force (RIF), on public sentiment, utilizing Reddit as a data source. We scraped the Reddit API from March 2nd to March 10th resulting in 557 unique posts with 12,553 comments from subreddits related to DOGE's approach to the RIF. Our exploratory analysis captures peaks in the number of posts/comments following major news event coverage related to the RIF, highlighting the utility of social media for understanding public perception of policy impacts. Our text analysis reveals a saliency on community with words such as people, employees, service, public, workers, and care being top words in both posts and comments. Moreover, a team of researchers coded a random subset of comments for public sentiment and found that only a quarter approved of DOGE's actions towards the RIF.
  
keywords:
  - Reddit
  - Federal Government
  - DOGE
editor: 
  markdown: 
    wrap: sentence
---

```{r include=FALSE}

knitr::opts_chunk$set(echo = FALSE)

library(readxl)
library(viridis)
library(ggthemes)
library(tidytext)
library(knitr)
library(tidyverse)
library(patchwork)


# path to repo
#repo_path <- str_c("~/Desktop/UMICH/SURV622_Assignment/data/") # felix
repo_path <- str_c("~/repos/SURV622_Assignment/data/") # kevin


data_files <- list.files(repo_path) |> 
  tibble() |> 
  rename(data_file_names = 1) |> 
  filter(str_detect(data_file_names, ".xlsx|.RDS")) |> 
  mutate(data_names = str_remove_all(data_file_names, "\\..*"))


list2env(
  map(data_files$data_file_names, function(x){
  
  # read data file
  if(str_detect(x, "posts_data_clean")) {
    dat = read_xlsx(str_c(repo_path, x))
  } 
    else if(str_detect(x, "comments_data_clean")){
    
    dat =  read_excel(str_c(repo_path, x),
                      col_types = c("text", "text", "date", "guess", "numeric",
                                    "numeric", "numeric", "numeric", "text",
                                     "guess", "text", "text", "guess", "text",
                                     "text"))
    
    }
    
  
    
    else(
    dat = readRDS(str_c(repo_path, x)) 
  )
  
  # manipualte variables
  dat = dat |> 
     mutate(date_utc = ymd(date_utc),
    hour_posted = hour(as_datetime(timestamp)),
  )

}) |> 
  # name the files
    set_names(data_files$data_names),

# save data to the global environment
globalenv()
)
```

## Introduction {#sec-intro}

Since its inception in January 2025, the Department of Government Efficiency (DOGE) has implemented significant reductions in the federal workforce, resulting in the separation of over 200,000 employees, in alignment with the new administration's campaign promises.
These reductions have generated widespread concern and anxiety among federal workers regarding their job security and mental well-being.
Reports indicate that DOGE's methods have raised questions about adherence to established internal policies.
The ongoing debate surrounding DOGE's impact on federal worker perceptions of job security necessitates a thorough examination.
This study proposes that Reddit, as a platform for public discourse, provides researchers with valuable insights into real-time reactions and discussions among federal workers on this topic.
Furthermore, this research aims to capture a range of public sentiments, including both critical and supportive perspectives, regarding DOGE's workforce reduction efforts.

## Methods {#sec-meth}

*Search Terms*.
Our initial Reddit API scrape facilitated the development of a targeted keyword search strategy for this research.
This iterative process resulted in the selection of eight keywords (see Table 1).
We excluded keywords deemed vague or overly broad, such as 'Elon Musk' or 'drain the swamp.' Additionally, we observed that certain keywords yielded a high volume of irrelevant posts.
For example, 'policy changes' returned numerous discussions unrelated to government workforce reductions.
Similarly, 'DOGE' produced results related to DOGEcoin, which is outside the scope of this study.
Therefore, we meticulously examined the subreddits identified in our initial scrape and selected those containing relevant discussions.
To ensure a balanced representation of viewpoints, we included subreddits with conservative perspectives, such as 'neoliberal' and 'AskConservatives,' while avoiding subreddits focused on specific agencies like 'NIH' or 'CDC,' which could skew the data.
In total, we selected 19 subreddits (see Appendix A) and utilized eight keywords, resulting in 152 unique keyword and subreddit combinations for our data collection.

```{r}
posts_data |> distinct(search_term) |> 
  kable(caption = "List of keywords related to topic")

```

*Analytic Plan*.
We collected data from the Reddit API hourly, from March 2nd through the 10th, by scraping each combination of our eight keywords and 19 subreddits, resulting in 152 API requests per hour.
Reddit API rate limits were managed by implementing a sleep timer between requests.
Raw data from new Reddit posts were entered into a data table without initial processing.
Following the data collection period, the raw dataset contained 643 posts.
Each row in the data table represented a single post, and the columns included the post's date and time, title, text, subreddit, comment count, URL, and the search term used for collection.
Duplicate posts, which occurred when multiple keywords matched the same post, were removed.
Post text was cleaned by removing weblinks and converting special characters, such as the at sign, to the word 'at.' The date column was converted to a date class variable, and the day of the week and time of day were extracted for analysis.
After cleaning, the dataset consisted of 557 posts.

A preliminary review of the posts revealed that many were factual statements, questions, or reposted news articles and videos, which were not suitable for sentiment coding.
Therefore, we also scraped the comments associated with each post.
This resulted in 12,553 comments after processing, including information on score, up/down votes, and date/time.
Comments containing only weblinks, videos, or images were removed.
A review of the comments revealed more opinionated statements and reactions.
We then randomly selected 400 comments (without replacement) and assigned them to four graduate students for coding, categorizing each comment as favoring, opposing, or neutral towards DOGE's approach to federal workforce reductions.

## Results

*Post and Comment Trends*

We show the number of posts pre/post processing by day in Figure 1 during the timeframe that we collected these data.
There appears to be a noticeable peak in activity during the middle of the week.
However, we don't have enough data across weeks to conclusively say it is due to the day of the week itself.
It is more likely that these spikes are linked to specific events that occurred on those days, which sparked conversations.
The two peaks in our graph could be attributed to two major news stories.
On March 5th, Elon Musk made a statement about wanting to "save Western civilization from empathy," and on March 6th, there were reports that President Trump was limiting Musk's authority due to backlash over cuts to DOGE.
These events likely had a significant impact on the volume of Reddit posts related to Elon Musk and DOGE, driving the observed spikes in activity.

On March 3rd, news outlets started reporting that DOGE was claiming \$105 billion dollars in savings from cutting "wasteful" spending by layoffs and cutting foreign aid.
This amount is controversial since since the receipts shown on their site only amount to less than [\$9.6 billion](https://abcnews.go.com/US/doge-website-now-saved-105-billion-backtracked-earlier/story?id=119408347).
This could have prompted Reddit users to take to subreddits and provide their opinion or thoughts on this matter.

```{r}
#| fig-width: 6
#| fig-height: 3
#| 
# number of total posts
total_posts <- posts_data |> 
  filter(date_utc > "2025-03-01") |> 
  group_by(date_utc) |> 
  reframe(total = n()) |> 
  mutate(Posts = "pre processing") |> 
  add_row(
    posts_data_clean |> 
  group_by(date_utc) |> 
  reframe(total = n()) |> 
  mutate(Posts = "post processing")
  )

total_posts |> 
  ggplot( aes(x = date_utc, y = total, color= Posts)) + 
  geom_point(size = 3) +  
  geom_line(linewidth = 1) +  
  labs(
    title = "Figure 1. Total Number of Daily Posts Pre/Post Processing",
    x = "Day",
    y = "Total Number of Posts") +
  scale_color_viridis_d(option="C", end=.8) + 
  theme_hc()
  

```

We see a peak in the comments in Figure 2 on March 6th, which was a Thursday, and perhaps in anticipation of DOGE policies that often are released on Friday morning and have come to be known as "[Musk-acre Friday](https://smotus.substack.com/p/friday-night-musk-acre)."

```{r}

comments_data_clean |> 
  group_by(date_utc) |> 
  reframe(total = n()) |> 
  ggplot( aes(x = date_utc, y = total)) + 
  geom_point(size = 3) +  
  geom_line(linewidth = 1) +  
  labs(
    title = "Figure 2. Total Number of daily comments",
    x = "Day",
    y = "Total Number of Posts") +
  scale_color_viridis_d(option="A") +
  theme_hc()
```

*Word Frequency*

Most posts occur during the early morning, noon, and afternoon hours (see Figure 3).
The highest peak is around lunchtime, likely due to people posting during their lunch break.
There is also significant activity in the afternoon, evening, and early morning.
It is possible that people have more time in the evenings allowing them to be more likely to post on Reddit.
The fewest posts are seen in the morning, which makes sense as people are typically getting ready for work, commuting, or sleeping in.
The trend for comments (see Figure 3) mirrors that of posts.
This makes sense because people are likely using Reddit at similar times for both posting and commenting.
However, the volume of comments is much higher than the number of posts, as each post receives many comments from different users.

```{r}
# Aggregate posts data
posts_by_hour <- posts_data_clean |> 
  group_by(hour_posted) |> 
  reframe(Total = n()) |> 
  mutate(type = "Posts")

# Aggregate comments data
comments_by_hour <- comments_data_clean |> 
  group_by(hour_posted) |> 
  reframe(Total = n()) |> 
  mutate(type = "Comments")

# Combine both datasets
combined_data <- bind_rows(comments_by_hour, posts_by_hour)

# Plot with facet_wrap to display them side by side
ggplot(combined_data, aes(x = hour_posted, y = Total)) + 
  geom_point(aes(color = type), size = 3) +  
  geom_line(aes(color = type), linewidth = 1) +
  scale_color_manual(values = c("Posts" = "steelblue", "Comments" = "lightgreen")) +  # Color distinction
  facet_wrap(~type, scales = "free_y") +  # Display side by side
  labs(
    title = "Figure 3. Total Number of Posts and Comments by Hour",
    subtitle = "(24-hour clock)",
    x = "Hour of Day",
    y = "Total Count"
  ) +
  theme_minimal() +
  theme_hc() +
  theme(legend.position = "none") 
```

The graphs below (Figure 4) show the most frequent words used in both Reddit posts and their comment sections while excluding our keyword search terms.
These words were identified by tokenizing the text and removing stop words, then counting the occurrences of the most common terms.
The removal of stop words allowed us to focus on contextual words in the posts/comments rather than common words like "the" and "and" that are not useful in these types of analysis.
Examining this visual reveals that in posts the current president's name is the top word.
However, in both posts and comments, there appears to be an emerging theme of community as words like people, employees, service, care, jobs, and public are also top words.
This highlights the focus of the community on these key themes, reflecting the ongoing conversations and interests of Reddit users within this context.

```{r}
#| fig-width: 6
#| fig-height: 6
#| 
keywords <- posts_data_clean |>
    select( search_term) |>
    pivot_longer(cols = search_term, values_drop_na = TRUE) |>
    unnest_tokens(word, value) |> 
  distinct(word) |> 
  pull()

# Tokenize words and count frequencies for Reddit posts
posts_word_counts <- posts_data_clean |>
  select( text) |>
  pivot_longer(cols = text, values_drop_na = TRUE) |>
  unnest_tokens(word, value) |>
  anti_join(stop_words, by = "word") |>
  count(word, sort = TRUE) |>
  # remove keywords
  filter(!(word %in% keywords),
         # filter out numbers
         !str_detect(word, "[:digit:]") ) |> 
  slice_max(n, n = 30)

# Tokenize words and count frequencies for Reddit comments
comments_word_counts <- comments_data_clean |>
  select(comment) |>
  pivot_longer(cols = comment, values_drop_na = TRUE) |>
  unnest_tokens(word, value) |>
  anti_join(stop_words, by = "word") |>
  count(word, sort = TRUE) |>
  # remove keywords
  filter(!(word %in% keywords),
         # filter out numbers
         !str_detect(word, "[:digit:]") ) |> 
  slice_max(order_by = n, n = 30)

# Plot for posts
plot_posts <- ggplot(posts_word_counts, aes(x = reorder(word, n), y = n)) +
  geom_col(fill = "steelblue") +
  coord_flip() +
  labs(x = "Word",
       y = "Count") +
  theme_minimal()

# Plot for comments
plot_comments <- ggplot(comments_word_counts, aes(x = reorder(word, n), y = n)) +
  geom_col(fill = "lightgreen") +
  coord_flip() +
  labs(x = "Word",
       y = "Count") +
  theme_minimal()

# Combine them side by side using patchwork
combined_plot <- plot_posts + plot_comments + plot_layout(ncol = 2) + plot_annotation(title = "Figure 4. Top 25 Words in Reddit Posts and Comments")

# Display the combined plot
combined_plot

```

*Coding Comments*

We randomly selected and coded 400 comments from the Reddit posts.
Table 2 shows the distribution of favorability coding for these comments.
Among the 400 comments, the majority (217) expressed opposition to DOGE's approach to reducing the federal workforce, while 110 were supportive, and 73 remained neutral.
A large number of comments (12,153) were not coded, as their content was not relevant for favorability assessment.

```{r}
comments_data_clean |> 
  mutate(outcome = replace_na(outcome, "Not Coded")) |> 
  count(outcome, name = 'Total') |> 
  
  kable(caption = "Distribution of favorability coding for 400 comments")
```

## Conclusion

In this study, we explored the impact of the DOGE's actions towards the reduction in force impacting federal workers’ perceptions of job security, using Reddit as a platform to analyze public discourse.
In examining Reddit posts and comments related to DOGE’s workforce reduction policies, we gained valuable insights into the sentiments of both federal workers and the broader public.
Our findings indicate that these discussions are heavily influenced by political events and high-profile figures, such as President Trump and Elon Musk, with particular attention given to DOGE’s controversial cost-cutting measures.
Through the analysis of both posts and comments, we captured a wide range of reactions, with the majority being strongly opposed to DOGE’s approach.
The frequent mention of terms related to the administration underscores the community’s focus on government policies and the emotional responses these issues generate.

In terms of favorability, the distribution of comments revealed that 217 out of 400 were opposed to DOGE’s approach, 110 were supportive, and 73 were neutral.
Additionally, patterns in post volume, especially peaks during significant news events, highlight the dynamic nature of these discussions, providing insights into the timing and public reactions to key policy decisions.
While our study provides a snapshot of Reddit discussions during a specific period, the findings demonstrate the value of social media platforms like Reddit in capturing public sentiment on contemporary political and policy issues.
However, it is important to note that Reddit may not fully represent the broader population’s views, as the platform tends to skew younger and more politically engaged, potentially limiting the generalizability of these findings.

The use of sentiment analysis on social media presents challenges, as it can be influenced by sarcasm, humor, or polarized language, which might affect the accuracy of sentiment classification.
Therefore, our research team hand coded a subset of comments, yet inter-rater reliability was not estimated.
The specific subreddits analyzed, while designed to capture diverse perspectives, may still over-represent certain political or ideological viewpoints.
Moreover, the relatively short timeframe of this analysis—spanning only March 2nd to March 10th—means that we may not have captured long-term trends or shifts in sentiment as the DOGE policies unfold.
Future research could address these limitations by expanding the data collection to other platforms or a longer timeframe and by incorporating demographic data to better understand the views of different Reddit user groups.

\newpage

## Appendix

We examined almost 100 subreddit posts while paying attention to the post topics and determined which ones were related to our topic.
We also included right leaning subreddits to attempt to capture a balanced public sentiment on the topic.
The following is our final list of subreddits that we scraped from.

```{r}
posts_data |> distinct(subreddit) |> 
  kable(caption = "List of subreddits relevant to the topic of DOGE")
```
